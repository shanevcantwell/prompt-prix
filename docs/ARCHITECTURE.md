# Architecture

prompt-prix is a visual fan-out MCP service: identical data dispatched across multiple LLMs simultaneously. Both a Gradio UI (for humans) and an MCP protocol server (for agents) consume the same stateless tool layer.

## Four-Layer Architecture

Per [ADR-006](adr/006-adapter-resource-ownership.md), every import in the codebase follows this strict layer model:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        ORCHESTRATION                            ‚îÇ
‚îÇ  BatteryRunner ‚îÇ ConsistencyRunner ‚îÇ ComparisonSession          ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚Ä¢ Zero mode awareness ‚Äî doesn't know react from single-shot   ‚îÇ
‚îÇ  ‚Ä¢ Calls execute_test_case(), receives CaseResult              ‚îÇ
‚îÇ  ‚Ä¢ Controls concurrency, validation pipeline (refusal ‚Üí drift) ‚îÇ
‚îÇ  ‚Ä¢ NEVER IMPORTS: adapters/*, ServerPool, ConcurrentDispatcher  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ execute_test_case(test, model_id, ...)
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     DISPATCH (react/dispatch.py)                ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  execute_test_case() ‚Äî the ONLY place that reads test.mode      ‚îÇ
‚îÇ    mode=None    ‚Üí _execute_single_shot() ‚Üí complete_stream()    ‚îÇ
‚îÇ    mode="react" ‚Üí _execute_react() ‚Üí react_step() √ó N          ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Returns CaseResult(response, latency_ms, react_trace)          ‚îÇ
‚îÇ  Raises ReactLoopIncomplete on cycle / max_iterations           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ MCP tool call
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       MCP PRIMITIVES                            ‚îÇ
‚îÇ  complete_stream ‚îÇ react_step ‚îÇ judge ‚îÇ drift ‚îÇ list_models     ‚îÇ
‚îÇ  geometry (analyze/generate variants)                           ‚îÇ
‚îÇ  trajectory (analyze/compare trajectories)                      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚Ä¢ Receives adapter via registry (get_adapter())                ‚îÇ
‚îÇ  ‚Ä¢ Stateless ‚Äî no mode awareness                                ‚îÇ
‚îÇ  ‚Ä¢ Exposed over JSON-RPC via server.py (prompt-prix-mcp)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ adapter.stream_completion()
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       ADAPTER LAYER                             ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  LMStudioAdapter                                                ‚îÇ
‚îÇ    INTERNAL: ServerPool, ConcurrentDispatcher, httpx            ‚îÇ
‚îÇ    STRATEGY: Multi-GPU parallel dispatch                        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  HuggingFaceAdapter                                             ‚îÇ
‚îÇ    INTERNAL: API client, rate limiter                           ‚îÇ
‚îÇ    STRATEGY: Rate-limited cloud calls                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Layer Import Rules

| Layer | MAY Import | MUST NOT Import |
|-------|------------|-----------------|
| **Orchestration** (BatteryRunner, ConsistencyRunner, ComparisonSession) | `react.dispatch`, `mcp.tools.*`, `mcp.registry` | `adapters/*`, ServerPool, ConcurrentDispatcher |
| **Dispatch** (`react/dispatch.py`) | `mcp.tools.*`, `react.schemas`, `react.cycle_detection` | `adapters/*`, orchestration |
| **MCP Primitives** | `adapters.base.HostAdapter` (protocol), `mcp.registry` | Concrete adapter classes, ServerPool |
| **Adapters** | httpx, internal utilities | Nothing from orchestration or MCP |

> **THE RULE:** ServerPool and ConcurrentDispatcher are INTERNAL to LMStudioAdapter.
> No file outside `adapters/lmstudio.py` may import or reference them.

## Entry Points

Both entry points bootstrap with `register_default_adapter()` and consume `mcp/tools/*`:

| Command | Module | Audience | Transport |
|---------|--------|----------|-----------|
| `prompt-prix` | `main.py` | Humans | Gradio web UI |
| `prompt-prix-mcp` | `mcp/server.py` | Agents | MCP stdio (JSON-RPC) |

`server.py` registers 9 tools with FastMCP via `add_tool()`. Agents (LAS, Claude Desktop, any MCP client) launch `prompt-prix-mcp` as a subprocess.

## Directory Structure

```
prompt_prix/
‚îú‚îÄ‚îÄ main.py              # Gradio UI entry point (prompt-prix command)
‚îú‚îÄ‚îÄ ui.py                # Gradio UI definition
‚îú‚îÄ‚îÄ handlers.py          # Shared event handlers (fetch, stop)
‚îú‚îÄ‚îÄ state.py             # Global mutable state
‚îú‚îÄ‚îÄ core.py              # ComparisonSession (orchestration)
‚îú‚îÄ‚îÄ config.py            # Pydantic models, constants, env loading
‚îú‚îÄ‚îÄ parsers.py           # Input parsing utilities
‚îú‚îÄ‚îÄ export.py            # Report generation
‚îú‚îÄ‚îÄ battery.py           # BatteryRunner (orchestration) - calls execute_test_case()
‚îú‚îÄ‚îÄ consistency.py       # ConsistencyRunner - multi-run variance testing
‚îú‚îÄ‚îÄ react/               # ReAct loop execution
‚îÇ   ‚îú‚îÄ‚îÄ dispatch.py      # execute_test_case() ‚Äî single dispatch (ONLY mode reader)
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py       # ReActIteration, ToolCall data models
‚îÇ   ‚îî‚îÄ‚îÄ cycle_detection.py # Stagnation / cycle detection
‚îú‚îÄ‚îÄ mcp/
‚îÇ   ‚îú‚îÄ‚îÄ server.py        # MCP protocol server (FastMCP over stdio) ‚Äî agent entry point
‚îÇ   ‚îú‚îÄ‚îÄ registry.py      # Adapter registry + register_default_adapter()
‚îÇ   ‚îî‚îÄ‚îÄ tools/
‚îÇ       ‚îú‚îÄ‚îÄ complete.py  # complete, complete_stream, latency sentinel utilities
‚îÇ       ‚îú‚îÄ‚îÄ react_step.py # Stateless single ReAct iteration primitive
‚îÇ       ‚îú‚îÄ‚îÄ drift.py     # Embedding-based semantic drift calculation
‚îÇ       ‚îú‚îÄ‚îÄ geometry.py  # Prompt variant generation and distance analysis
‚îÇ       ‚îú‚îÄ‚îÄ trajectory.py # Semantic velocity/acceleration analysis
‚îÇ       ‚îú‚îÄ‚îÄ judge.py     # LLM-as-judge evaluation
‚îÇ       ‚îú‚îÄ‚îÄ list_models.py
‚îÇ       ‚îî‚îÄ‚îÄ _semantic_chunker.py  # Shared helpers for semantic-chunker tools
‚îú‚îÄ‚îÄ tabs/
‚îÇ   ‚îú‚îÄ‚îÄ battery/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ handlers.py  # Battery-specific handlers
‚îÇ   ‚îî‚îÄ‚îÄ compare/
‚îÇ       ‚îî‚îÄ‚îÄ handlers.py  # Compare-specific handlers
‚îú‚îÄ‚îÄ adapters/
‚îÇ   ‚îú‚îÄ‚îÄ base.py          # HostAdapter protocol
‚îÇ   ‚îú‚îÄ‚îÄ lmstudio.py      # LMStudioAdapter (OWNS ServerPool, ConcurrentDispatcher)
‚îÇ   ‚îî‚îÄ‚îÄ huggingface.py   # HuggingFaceAdapter (rate-limited cloud calls)
‚îú‚îÄ‚îÄ semantic_validator.py # Response validation (refusals, tool calls, verdicts)
‚îî‚îÄ‚îÄ benchmarks/
    ‚îú‚îÄ‚îÄ base.py          # BenchmarkCase dataclass
    ‚îú‚îÄ‚îÄ custom_json.py   # CustomJSONLoader (JSON/JSONL)
    ‚îî‚îÄ‚îÄ promptfoo.py     # PromptfooLoader (YAML format)
```

## Adapter Layer

All adapters implement the `HostAdapter` protocol:

```python
class HostAdapter(Protocol):
    async def get_available_models(self) -> list[str]: ...
    async def stream_completion(self, task: InferenceTask) -> AsyncGenerator[str, None]: ...
```

ServerPool and ConcurrentDispatcher are LM Studio concepts. Other backends have different resource models. The adapter encapsulates its internals ‚Äî orchestration never sees these classes.

## Timeout Contract

A single MCP tool call (e.g. `complete`) passes through three layers, each with different timeout semantics:

```
MCP Client (LAS)          prompt-prix              LM Studio
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ              ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
client timeout_ms    ‚Üí    no MCP-layer timeout  ‚Üí  httpx timeout
(client controls)         (FastMCP has none)       (= task.timeout_seconds)
```

| Layer | Timeout | Default | Scope |
|-------|---------|---------|-------|
| **MCP transport** | None | ‚Äî | FastMCP imposes no timeout. The client (LAS) must set its own `timeout_ms` on the MCP call. |
| **Dispatcher queue** | **Unbounded** | ‚Äî | `ConcurrentDispatcher.submit()` awaits a server slot with no timeout. If all servers are busy, the call blocks until one frees up. This is intentional: queue wait is excluded from latency measurement. |
| **HTTP inference** | `task.timeout_seconds` | 300s (`complete`), 60s (`InferenceTask` default) | Applied to the httpx client. Covers connection + streaming from LM Studio. |

### Implications for MCP clients

**Single primitive call** (`complete`, `judge`, `react_step`): Wall-clock time = queue wait + inference. With idle servers, queue wait is near-zero and 300s covers even large generations. With busy servers (battery running on the same adapter), queue wait could be minutes ‚Äî the call blocks in the dispatcher until a slot opens.

**`react_step` in a loop**: Each step is one MCP call. Total wall-clock for an N-step react loop = N √ó (queue wait + inference). The MCP client controls the loop and can bail out at any point.

**Battery orchestration** (future `run_battery` tool): Would dispatch the entire test matrix internally. Could run 5-30 minutes. A single MCP tool call sitting open that long is architecturally awkward. Options when this becomes needed:
1. **Progress notifications** via MCP notifications (MCP protocol supports `notifications/progress`)
2. **Async pattern**: `start_battery` returns a run ID, `poll_battery` checks status
3. **Keep it out of MCP**: Battery is an orchestration concern ‚Äî run via Gradio UI or a script, not as an MCP tool

### What happens on MCP connection drop

If the MCP client times out or disconnects while a tool call is in-flight:
- The stdio pipe closes
- FastMCP's event loop exits
- Any in-flight `await` (dispatcher queue or httpx stream) raises `CancelledError`
- `ConcurrentDispatcher.submit()` handles cancellation: if a server was already acquired, it's released back to the pool (lines 184-196 of `lmstudio.py`)
- No orphaned state ‚Äî the adapter cleans up

### Setting client-side timeout

For LAS or other MCP clients, recommended `timeout_ms`:

| Tool | Recommended | Rationale |
|------|-------------|-----------|
| `list_models` | 30s | Network round-trip to each server |
| `complete` | 600s | 300s inference + up to 300s queue wait |
| `react_step` | 600s | Same as `complete` (one inference call) |
| `judge` | 600s | Same as `complete` (uses LLM inference internally) |
| `calculate_drift` | 10s | Near-instant embedding cosine distance |
| `analyze_variants`, `analyze_trajectory`, `compare_trajectories` | 10s | Embedding-based, no LLM inference |
| `generate_variants` | 600s | Uses LLM inference |

## Battery Execution: Pipelined Judging

When a judge model is selected, BatteryRunner uses **pipelined execution** ‚Äî judge tasks are submitted eagerly as inference results complete, rather than waiting for all inference to finish first:

```
Without pipelining (original two-phase, ADR-008):
  Phase 1: [inference][inference][inference][inference]
  Phase 2:                                              [judge][judge][judge][judge]

With pipelining:
  GPU0:    [inference][inference][judge][judge][judge]    ‚Üê GPU0 idles early, starts judging
  GPU1:    [inference][inference][inference][inference]   ‚Üê GPU1 still doing heavy models
```

The `current_model` drain guard on `ServerPool` is the enabler ‚Äî judge tasks queue in the dispatcher until a server drains its inference model. When no judge model is set, `_execute_inference_phase()` runs directly with no pipelining overhead.

See [ADR-008](adr/ADR-008-judge-scheduling-strategy.md) for the evolution from two-phase to pipelined scheduling.

## ReAct Loop Execution

Tests with `mode="react"` evaluate multi-step tool-use loops. The key design decision: **a react loop is just another way to produce a pass/fail verdict for a (test, model) cell.** React tests flow through the same orchestration pipeline as standard tests ‚Äî they get drift validation, judge evaluation, and consistency testing for free.

`execute_test_case()` in `react/dispatch.py` is the **only place** that reads `test.mode`. Orchestration above and MCP tools below have zero mode awareness.

The react loop:
1. Calls `react_step()` MCP primitive (stateless ‚Äî takes trace in, returns one step out)
2. Accumulates `ReActIteration` objects in the trace
3. Checks for stagnation via `detect_cycle_with_pattern()` after each step
4. Completes when the model responds with text only (no tool calls)
5. Raises `ReactLoopIncomplete` on cycle detection or `max_iterations` exhaustion

| Outcome | Result |
|---------|--------|
| Loop completes (final text answer) | `RunResult(COMPLETED)` |
| Cycle detected or max iterations | `RunResult(SEMANTIC_FAILURE)` |
| Infrastructure error | `RunResult(ERROR)` |

## Consistency Testing

`ConsistencyRunner` runs each (test, model) cell N times with different random seeds to identify models that produce inconsistent results.

| Status | Symbol | Meaning |
|--------|--------|---------|
| `CONSISTENT_PASS` | ‚úì | N/N runs passed |
| `CONSISTENT_FAIL` | ‚ùå | 0/N runs passed |
| `INCONSISTENT` | üü£ 3/5 | Some runs passed, some failed |

See [ADR-010](adr/ADR-010-consistency-runner.md) for rationale.

## Semantic Validation

Battery tests validate responses beyond HTTP success (`semantic_validator.py`):

| Check | Trigger | Failure Reason |
|-------|---------|----------------|
| **Empty response** | Response is empty/whitespace | "Empty response" |
| **Refusal detection** | Matches refusal phrases | "Model refused: '{phrase}'" |
| **Tool call required** | `tool_choice: "required"` | "Expected tool call but got text response" |
| **Tool call forbidden** | `tool_choice: "none"` | "Tool call made when tool_choice='none'" |
| **Verdict matching** | `pass_criteria` contains verdict | "Verdict mismatch: expected X, got Y" |

Checks run in order (first failure wins). Verdict matching enables judge competence tests ‚Äî testing whether a model can correctly judge other outputs.

| Status | Symbol | Meaning |
|--------|--------|---------|
| `COMPLETED` | ‚úì | Response passed semantic validation |
| `SEMANTIC_FAILURE` | ‚ùå | Response received but failed semantic check |
| `ERROR` | ‚ö† | Infrastructure error (timeout, connection, etc.) |

## Battery File Formats

**Required fields:** `id`, `user`

**Optional fields:** `name`, `category`, `severity`, `system`, `tools`, `tool_choice`, `mode`, `mock_tools`, `max_iterations`, `expected`, `pass_criteria`, `fail_criteria`, `expected_response`

**Formats:** JSON (with `prompts` array), JSONL (one per line), Promptfoo YAML (with `prompts` + `tests`).

Promptfoo vars extraction:

| Var | BenchmarkCase field | Purpose |
|-----|-------------------|---------|
| `expected_verdict` | `pass_criteria` | Rubric text for LLM judge evaluation |
| `expected_response` | `expected_response` | Exemplar text for embedding drift comparison |
| `category` | `category` | Test category for filtering/grouping |
| `system` | `system` | System message |
| `user` | `user` | User message |

Promptfoo `assert` blocks are logged but **not evaluated** (warning emitted).

## Integration Points

All inference servers must expose OpenAI-compatible endpoints (`GET /v1/models`, `POST /v1/chat/completions`). Supported: LM Studio, Ollama, vLLM, llama.cpp server, any OpenAI-compatible proxy. See [ADR-003](adr/003-openai-compatible-api.md).

## Architecture Decision Records

| ADR | Decision |
|-----|----------|
| [001](adr/001-use-existing-benchmarks.md) | Use existing benchmarks (BFCL, Inspect AI) instead of custom eval schema |
| [002](adr/002-fan-out-pattern-as-core.md) | Fan-out pattern as core architectural abstraction |
| [003](adr/003-openai-compatible-api.md) | OpenAI-compatible API as sole integration layer |
| [006](adr/006-adapter-resource-ownership.md) | Adapters own their resource management (ServerPool internal to LMStudioAdapter) |
| [007](adr/ADR-007-inference-task-schema.md) | InferenceTask schema for adapter interface |
| [008](adr/ADR-008-judge-scheduling-strategy.md) | Pipelined judge scheduling for multi-GPU efficiency |
| [009](adr/ADR-009-interactive-battery-grid.md) | Dismissible dialog for battery grid cell detail |
| [010](adr/ADR-010-consistency-runner.md) | Multi-run consistency analysis (proposed) |
| [011](adr/ADR-011-embedding-based-validation.md) | Embedding-based semantic validation (proposed) |
| [012](adr/ADR-012-compare-to-battery-export.md) | Compare to Battery export pipeline (proposed) |
| [013](adr/ADR-013-semantic-chunker-mcp-primitives.md) | Semantic-chunker MCP primitives (geometry, trajectory) |
| 014 | MCP protocol server ‚Äî FastMCP over stdio for agent access |
