# Architecture

This document describes the system architecture of prompt-prix, including module responsibilities, data flow, and key design decisions.

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Browser                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Gradio UI (ui.py)                        â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚ Config Panelâ”‚ â”‚ Prompt Input â”‚ â”‚ Model Output Tabs     â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Servers   â”‚ â”‚ â€¢ Single     â”‚ â”‚ â€¢ Tab 1..10           â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Models    â”‚ â”‚ â€¢ Batch      â”‚ â”‚ â€¢ Streaming display   â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ System    â”‚ â”‚ â€¢ Tools JSON â”‚ â”‚ â€¢ Status colors       â”‚ â”‚   â”‚
â”‚  â”‚  â”‚   Prompt    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚ localStorage: servers, models, temperature, etc.        â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Python Backend                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    handlers.py (Orchestration)                â”‚  â”‚
â”‚  â”‚  â€¢ fetch_available_models()  â†’ adapter.get_available_models() â”‚  â”‚
â”‚  â”‚  â€¢ initialize_session()      â†’ Create ComparisonSession       â”‚  â”‚
â”‚  â”‚  â€¢ send_single_prompt()      â†’ adapter.stream_completion()    â”‚  â”‚
â”‚  â”‚  â€¢ export_markdown/json()    â†’ Report generation              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                â”‚                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              adapters/ (Resource Management)                  â”‚  â”‚
â”‚  â”‚                             â”‚                                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚  LMStudioAdapter                                         â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ _pool: ServerPool (internal)                          â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ stream_completion() â†’ finds server, streams, releases â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ get_available_models() â†’ queries all servers          â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  Future: HFInferenceAdapter, SurfMcpAdapter                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                â”‚                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    config.py                                  â”‚  â”‚
â”‚  â”‚  Pydantic Models: ServerConfig, ModelContext, SessionState   â”‚  â”‚
â”‚  â”‚  Constants: DEFAULT_TEMPERATURE, DEFAULT_MAX_TOKENS, etc.    â”‚  â”‚
â”‚  â”‚  Environment: load_servers_from_env(), get_gradio_port()     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LM Studio Servers                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Server 1 (e.g. 3090)  â”‚    â”‚  Server 2 (e.g. 8000)  â”‚          â”‚
â”‚  â”‚  â€¢ GET /v1/models      â”‚    â”‚  â€¢ GET /v1/models      â”‚          â”‚
â”‚  â”‚  â€¢ POST /v1/chat/...   â”‚    â”‚  â€¢ POST /v1/chat/...   â”‚          â”‚
â”‚  â”‚  â””â”€ Model A            â”‚    â”‚  â””â”€ Model B, C         â”‚          â”‚
â”‚  â”‚  â””â”€ Model B            â”‚    â”‚                        â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Layer Import Rules

Per [ADR-006](adr/006-adapter-resource-ownership.md), the codebase has strict layer boundaries:

| Layer | MAY Import | MUST NOT Import |
|-------|------------|-----------------|
| **Orchestration** (BatteryRunner, ComparisonSession) | `mcp.tools.*`, `mcp.registry` | `adapters/*`, ServerPool, ConcurrentDispatcher |
| **MCP Primitives** | `adapters.base.HostAdapter` (protocol), `mcp.registry` | Concrete adapter classes, ServerPool |
| **Adapters** | httpx, internal utilities | Nothing from orchestration or MCP |

> **THE RULE:** ServerPool and ConcurrentDispatcher are INTERNAL to LMStudioAdapter.
> No file outside `adapters/lmstudio.py` may import or reference them.

## Module Breakdown

### Directory Structure

```
prompt_prix/
â”œâ”€â”€ main.py              # Entry point, adapter registration
â”œâ”€â”€ ui.py                # Gradio UI definition
â”œâ”€â”€ handlers.py          # Shared event handlers (fetch, stop)
â”œâ”€â”€ state.py             # Global mutable state
â”œâ”€â”€ core.py              # ComparisonSession (orchestration)
â”œâ”€â”€ config.py            # Pydantic models, constants, env loading
â”œâ”€â”€ parsers.py           # Input parsing utilities
â”œâ”€â”€ export.py            # Report generation
â”œâ”€â”€ battery.py           # BatteryRunner (orchestration) - calls MCP tools
â”œâ”€â”€ consistency.py       # ConsistencyRunner - multi-run variance testing
â”œâ”€â”€ mcp/
â”‚   â”œâ”€â”€ registry.py      # Adapter registry (get_adapter, register_adapter)
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ complete.py  # complete, complete_stream primitives
â”‚       â”œâ”€â”€ judge.py     # LLM-as-judge evaluation
â”‚       â””â”€â”€ list_models.py
â”œâ”€â”€ tabs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ battery/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ handlers.py  # Battery-specific handlers
â”‚   â””â”€â”€ compare/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ handlers.py  # Compare-specific handlers
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ base.py          # HostAdapter protocol
â”‚   â””â”€â”€ lmstudio.py      # LMStudioAdapter (OWNS ServerPool, ConcurrentDispatcher)
â”œâ”€â”€ semantic_validator.py # Response validation (refusals, tool calls, verdicts)
â””â”€â”€ benchmarks/
    â”œâ”€â”€ base.py          # BenchmarkCase dataclass
    â”œâ”€â”€ custom_json.py   # CustomJSONLoader (JSON/JSONL)
    â””â”€â”€ promptfoo.py     # PromptfooLoader (YAML format)
```

### config.py - Configuration & Data Models

**Purpose**: Define all Pydantic models for type-safe configuration and state.

| Class | Purpose |
|-------|---------|
| `ServerConfig` | Single LM Studio server state (URL, available_models, is_busy) |
| `ModelConfig` | Model identity and display name |
| `Message` | Single message in a conversation (role, content - supports multimodal) |
| `ModelContext` | Complete conversation history for one model |
| `SessionState` | Full session: models, contexts, system_prompt, halted status |

**Message Multimodal Support**:
The `Message` model supports both text and multimodal content:
```python
# Text-only message
Message(role="user", content="Hello")

# Multimodal message (text + image)
Message(role="user", content=[
    {"type": "text", "text": "What's in this image?"},
    {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}
])

# Helper methods
msg.get_text()   # Extract text content
msg.has_image()  # Check if message contains an image
```

**Key Functions**:
- `load_servers_from_env()` - Read LM_STUDIO_SERVER_N environment variables
- `get_default_servers()` - Return env servers or placeholder defaults
- `get_gradio_port()` - Read GRADIO_PORT or default to 7860
- `encode_image_to_data_url(path)` - Convert image file to base64 data URL
- `build_multimodal_content(text, image_path)` - Build OpenAI-format multimodal content

### core.py - Session Management (Orchestration Layer)

**Purpose**: Orchestration-level session management.

#### ComparisonSession

Manages a comparison session. Calls MCP tools, not adapters directly.

```python
class ComparisonSession:
    state: SessionState  # Contains models, contexts, config

    async def send_prompt_to_model(model_id, prompt, on_chunk=None)
    async def send_prompt_to_all(prompt, on_chunk=None)
    def get_context_display(model_id) -> str
```

### handlers.py - Shared Event Handlers

**Purpose**: Shared async handlers used across multiple tabs.

| Handler | Purpose | Returns |
|---------|---------|---------|
| `fetch_available_models(servers_text)` | Query all servers for available models | `(status, gr.update(choices=[...]))` |
| `handle_stop()` | Signal cancellation via global state | `status` |
| `_init_pool_and_validate(servers_text, models)` | Initialize ServerPool and validate models | `(pool, error_message)` |

### tabs/battery/handlers.py - Battery Tab Handlers

**Purpose**: Handlers specific to the Battery (benchmark) tab.

| Handler | Trigger | Returns |
|---------|---------|---------|
| `validate_file(file_path)` | File upload | Validation status string |
| `get_test_ids(file_path)` | File upload | List of test IDs |
| `run_handler(file, models, servers, ...)` | "Run Battery" button | Generator yielding `(status, grid_df)` |
| `quick_prompt_handler(prompt, models, ...)` | "Run Prompt" button | Markdown results |
| `export_json()` | "Export JSON" button | `(status, preview)` |
| `export_csv()` | "Export CSV" button | `(status, preview)` |
| `get_cell_detail(model, test)` | Detail dropdown | Markdown detail |
| `refresh_grid(display_mode)` | Display mode change | Updated grid DataFrame |

### tabs/compare/handlers.py - Compare Tab Handlers

**Purpose**: Handlers specific to the Compare (interactive) tab.

| Handler | Trigger | Returns |
|---------|---------|---------|
| `initialize_session(servers, models, system_prompt, ...)` | Auto-init on send | `(status, *model_tabs)` |
| `send_single_prompt(prompt, tools_json, image_path, seed, repeat_penalty)` | "Send to All" button | Generator yielding `(status, tab_states, *model_outputs)` |
| `export_markdown()` | "Export Markdown" button | `(status, preview)` |
| `export_json()` | "Export JSON" button | `(status, preview)` |
| `launch_beyond_compare(model_a, model_b)` | "Open in Beyond Compare" button | `status` |

**Compare Tab Features**:
- **Image Attachment**: Upload images for vision models (encoded as base64 data URLs)
- **Seed Parameter**: Set a seed for reproducible outputs across models
- **Repeat Penalty**: Configurable penalty (1.0-2.0) to reduce repetitive token generation

### ui.py - Gradio UI Definition

**Purpose**: Define all Gradio components and wire up event bindings.

**Key Components**:

| Component | Type | Purpose |
|-----------|------|---------|
| `servers_input` | Textbox | LM Studio server URLs (one per line) |
| `models_checkboxes` | CheckboxGroup | Select models to compare |
| `system_prompt_input` | Textbox (50 lines) | Editable system prompt |
| `temperature_slider` | Slider | Model temperature (0-2) |
| `timeout_slider` | Slider | Request timeout (30-600s) |
| `max_tokens_slider` | Slider | Max tokens (256-8192) |
| `seed_input` | Number | Optional seed for reproducible outputs |
| `repeat_penalty_slider` | Slider | Repeat penalty (1.0-2.0, default 1.1) |
| `prompt_input` | Textbox | User prompt entry |
| `image_input` | Image | Optional image attachment for vision models |
| `tools_input` | Code (JSON) | Tools for function calling |
| `model_outputs[0..9]` | Markdown | Model response tabs |
| `tab_states` | JSON (hidden) | Tab status for color updates |

**Event Bindings**:
- Buttons trigger async handlers
- `tab_states.change` triggers JavaScript for inline style updates
- `app.load` restores state from localStorage

### state.py - Global State

**Purpose**: Holds mutable state shared across handlers.

```python
session: Optional[ComparisonSession] = None
```

**Design Decision**: Separated to avoid circular imports between ui.py and handlers.py.

### adapters/ - Inference Provider Adapters

**Purpose**: Encapsulate backend-specific logic behind a uniform interface.

Per [ADR-006](adr/006-adapter-resource-ownership.md), the architecture has three strict layers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ORCHESTRATION                            â”‚
â”‚  BatteryRunner â”‚ ConsistencyRunner â”‚ ComparisonSession          â”‚
â”‚                                                                 â”‚
â”‚  â€¢ Calls MCP primitives ONLY â€” never adapters directly          â”‚
â”‚  â€¢ Controls concurrency via semaphore                           â”‚
â”‚  â€¢ NEVER IMPORTS: adapters/*, ServerPool, ConcurrentDispatcher  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ MCP tool call
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       MCP PRIMITIVES                            â”‚
â”‚  complete â”‚ complete_stream â”‚ judge â”‚ list_models               â”‚
â”‚                                                                 â”‚
â”‚  â€¢ Receives adapter via registry (get_adapter())                â”‚
â”‚  â€¢ Stateless pass-through                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ adapter.stream_completion()
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       ADAPTER LAYER                             â”‚
â”‚                                                                 â”‚
â”‚  LMStudioAdapter                                                â”‚
â”‚    INTERNAL: ServerPool, ConcurrentDispatcher, httpx            â”‚
â”‚    STRATEGY: Multi-GPU parallel dispatch                        â”‚
â”‚                                                                 â”‚
â”‚  SurfMcpAdapter                                                 â”‚
â”‚    INTERNAL: browser session                                    â”‚
â”‚    STRATEGY: Sequential (one browser)                           â”‚
â”‚                                                                 â”‚
â”‚  HFInferenceAdapter                                             â”‚
â”‚    INTERNAL: API client, rate limiter                           â”‚
â”‚    STRATEGY: Rate-limited cloud calls                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### HostAdapter Protocol

```python
class HostAdapter(Protocol):
    async def get_available_models(self) -> list[str]: ...
    async def stream_completion(
        self,
        model_id: str,
        messages: list[dict],
        temperature: float,
        max_tokens: int,
        timeout_seconds: int,
        tools: Optional[list[dict]] = None
    ) -> AsyncGenerator[str, None]: ...
```

#### LMStudioAdapter

```python
class LMStudioAdapter:
    def __init__(self, server_urls: list[str]):
        # ServerPool and ConcurrentDispatcher are INTERNAL
        self._pool = ServerPool(server_urls)
        self._dispatcher = ConcurrentDispatcher(self._pool)

    async def stream_completion(...) -> AsyncGenerator[str, None]:
        # Finds available server, acquires it, streams, releases
```

**Key Principle**: ServerPool and ConcurrentDispatcher are LM Studio concepts. Other backends have different resource models. The adapter encapsulates this â€” orchestration never sees these classes.

### parsers.py - Text Parsing Utilities

**Purpose**: Parse user input from UI components.

| Function | Input | Output |
|----------|-------|--------|
| `parse_models_input(text)` | "model1\nmodel2" | `["model1", "model2"]` |
| `parse_servers_input(text)` | "http://...\nhttp://..." | `["http://...", "http://..."]` |
| `parse_prompts_file(content)` | File content | List of prompts |
| `load_system_prompt(file_path)` | Optional file path | System prompt string |
| `get_default_system_prompt()` | - | Default prompt from file or constant |

### export.py - Report Generation

**Purpose**: Generate exportable reports from session state.

```python
def generate_markdown_report(state: SessionState) -> str:
    """Create Markdown with header, system prompt, and all model conversations."""

def generate_json_report(state: SessionState) -> str:
    """Create structured JSON with configuration and conversations."""

def save_report(content: str, filepath: str):
    """Write report to file."""
```

### main.py - Entry Point

**Purpose**: Application entry point and backwards-compatibility exports.

```python
def run():
    app = create_app()
    app.launch(server_name="0.0.0.0", server_port=get_gradio_port())
```

## Data Flow: Sending a Prompt

```
1. User types prompt, clicks "Send Prompt"
         â”‚
         â–¼
2. ui.py: send_button.click(fn=send_single_prompt, inputs=[prompt, tools])
         â”‚
         â–¼
3. handlers.py: send_single_prompt(prompt, tools_json)
   â”‚ - Validate session exists
   â”‚ - Parse tools JSON
   â”‚ - Add user message to all model contexts
   â”‚ - Refresh server manifests
         â”‚
         â–¼
4. Concurrent Dispatcher Loop:
   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ â”‚ For each idle server:                   â”‚
   â”‚ â”‚   Find model in queue this server has   â”‚
   â”‚ â”‚   If found: start async task            â”‚
   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚ â”‚ await asyncio.sleep(0.1)
   â”‚ â”‚ yield (status, tab_states, *outputs)  â”€â”€â”€â”€â”€â”€â–º UI updates
   â”‚ â”‚ Clean up completed tasks
   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ while queue or active_tasks
         â”‚
         â–¼
5. Each async task: run_model_on_server(model_id, server_url)
   â”‚ - Mark model as "streaming"
   â”‚ - Call stream_completion() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º LM Studio API
   â”‚ - Accumulate chunks in streaming_responses[model_id]
   â”‚ - On complete: add assistant message to context
   â”‚ - Release server
         â”‚
         â–¼
6. Final yield: ("âœ… All responses complete", final_states, *final_outputs)
```

## State Management

### Session State (Python)

```python
SessionState:
  models: list[str]                    # Selected models
  contexts: dict[str, ModelContext]    # model_id -> conversation
  system_prompt: str
  temperature: float
  timeout_seconds: int
  max_tokens: int
  halted: bool                         # True if any model failed
  halt_reason: Optional[str]
```

### UI State (Browser localStorage)

| Key | Type | Purpose |
|-----|------|---------|
| `promptprix_servers` | string | Server URLs (newline-separated) |
| `promptprix_model_choices` | JSON array | Available models from last fetch |
| `promptprix_models` | JSON array | Selected models |
| `promptprix_temperature` | float | Temperature setting |
| `promptprix_timeout` | int | Timeout setting |
| `promptprix_max_tokens` | int | Max tokens setting |
| `promptprix_tools` | string | Tools JSON |
| `promptprix_system_prompt` | string | System prompt text |

**Persistence**: Only saved when user clicks "Save State" button (explicit save).

## Tab Status Visualization

Tab colors indicate model status during streaming:

| Status | Color | Border |
|--------|-------|--------|
| `pending` | Red gradient (#fee2e2 â†’ #fecaca) | 4px solid #ef4444 |
| `streaming` | Yellow gradient (#fef3c7 â†’ #fde68a) | 4px solid #f59e0b |
| `completed` | Green gradient (#d1fae5 â†’ #a7f3d0) | 4px solid #10b981 |

**Implementation**: Uses inline JavaScript styles (`element.style`) to overcome Gradio theme CSS.

## Error Handling

### Fail-Fast Validation

1. `initialize_session` validates:
   - Servers are configured
   - Models are configured
   - All selected models exist on at least one server

2. `send_single_prompt` validates:
   - Session is initialized
   - Session is not halted
   - Prompt is not empty
   - Tools JSON is valid (if provided)

### Halt-on-Error

If any model fails during `send_prompt_to_all`:
- `state.halted = True`
- `state.halt_reason = "Model {model_id} failed: {error}"`
- Subsequent prompts are rejected

### Human-Readable Errors

The `LMStudioError` exception extracts error messages from LM Studio's JSON responses:

```python
{"error": {"message": "Model not loaded"}}  â†’  "Model not loaded"
```

## Integration Points

### Upstream: Benchmark Sources

prompt-prix can consume test cases from established benchmark ecosystems:

| Source | Format | Usage |
|--------|--------|-------|
| **BFCL** | JSON with function schemas | Export test cases, load in batch mode |
| **Inspect AI** | Python test definitions | Export prompts, import as JSON |
| **Custom JSON** | OpenAI-compatible messages | Direct load in prompt-prix |

See [ADR-001](adr/001-use-existing-benchmarks.md) for rationale.

### API Layer: OpenAI-Compatible

All inference servers must expose OpenAI-compatible endpoints:

```
GET  /v1/models              â†’ List available models
POST /v1/chat/completions    â†’ Chat completion (streaming)
```

Supported servers:
- LM Studio (native)
- Ollama (OpenAI mode)
- vLLM
- llama.cpp server
- Any OpenAI-compatible proxy

See [ADR-003](adr/003-openai-compatible-api.md) for rationale.

## Battery File Formats

The Battery tab accepts test files in multiple formats:

### JSON / JSONL

```json
{
  "prompts": [
    {"id": "test-1", "user": "What is 2+2?", "expected": "4"},
    {"id": "test-2", "user": "Call get_weather", "tools": [...], "tool_choice": "required"}
  ]
}
```

**Required fields**: `id`, `user`

**Optional fields**: `name`, `category`, `severity`, `system`, `tools`, `tool_choice`, `expected`, `pass_criteria`, `fail_criteria`

### Promptfoo YAML

[Promptfoo](https://promptfoo.dev) config files are supported with variable substitution:

```yaml
prompts:
  - |
    {{system}}
    User: {{user}}

tests:
  - description: "Clear Pass - Exact Match"
    vars:
      system: "You are evaluating tool call outputs..."
      user: "Evaluate this output..."
      expected_verdict: PASS      # Used for semantic validation
      category: clear_discrimination
    assert:
      - type: javascript          # Logged but NOT evaluated
        value: "result.verdict === 'PASS'"
```

**Promptfoo-specific handling**:
- `vars.expected_verdict` â†’ Extracted to `pass_criteria` for semantic validation
- `vars.category` â†’ Extracted to test category for filtering
- `assert` blocks â†’ **Logged but NOT evaluated** (warning emitted)

See `prompt_prix/benchmarks/promptfoo.py` for implementation.

## Semantic Validation

Battery tests validate responses beyond HTTP success. The semantic validator (`prompt_prix/semantic_validator.py`) checks:

### Validation Types

| Check | Trigger | Failure Reason |
|-------|---------|----------------|
| **Empty response** | Response is empty/whitespace | "Empty response" |
| **Refusal detection** | Matches refusal phrases | "Model refused: '{phrase}'" |
| **Tool call required** | `tool_choice: "required"` | "Expected tool call but got text response" |
| **Tool call forbidden** | `tool_choice: "none"` | "Tool call made when tool_choice='none'" |
| **Verdict matching** | `pass_criteria` contains verdict | "Verdict mismatch: expected X, got Y" |

### Verdict Matching (Judge Competence Tests)

When `pass_criteria` contains "verdict must be", the validator extracts the verdict from JSON in the response and compares it:

```python
# pass_criteria: "The verdict in the JSON response must be 'FAIL'"
# Response: {"verdict": "PASS", "score": 1.0, "reasoning": "..."}
# Result: SEMANTIC_FAILURE - "Verdict mismatch: expected FAIL, got PASS"
```

This enables testing whether a model can correctly judge other outputs (judge competence tests).

### Test Status Values

| Status | Symbol | Meaning |
|--------|--------|---------|
| `COMPLETED` | âœ“ | Response passed semantic validation |
| `SEMANTIC_FAILURE` | âŒ | Response received but failed semantic check |
| `ERROR` | âš  | Infrastructure error (timeout, connection, etc.) |

### Validation Order

Checks run in order (first failure wins):
1. Empty response check
2. Refusal detection
3. Tool call validation (if `tool_choice` set)
4. Verdict matching (if `pass_criteria` specifies verdict)

## Fan-Out Dispatcher Pattern

The core abstraction is **fan-out**: one prompt dispatched to N models in parallel.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Fan-Out Dispatcher                       â”‚
â”‚                                                              â”‚
â”‚  Input: (prompt, [model_a, model_b, model_c])               â”‚
â”‚                        â”‚                                     â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚         â–¼              â–¼              â–¼                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚    â”‚ Model A â”‚    â”‚ Model B â”‚    â”‚ Model C â”‚               â”‚
â”‚    â”‚ Server1 â”‚    â”‚ Server1 â”‚    â”‚ Server2 â”‚               â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜               â”‚
â”‚         â”‚              â”‚              â”‚                     â”‚
â”‚         â–¼              â–¼              â–¼                     â”‚
â”‚    Response A     Response B     Response C                 â”‚
â”‚                                                              â”‚
â”‚  Output: {model_a: resp_a, model_b: resp_b, model_c: resp_c}â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Parallel Dispatch

The dispatcher maximizes GPU utilization:

1. **Queue**: All work items (model + test pairs)
2. **Match**: Find idle server that has the required model loaded
3. **Execute**: Stream response, update UI
4. **Release**: Server becomes available for next item

This keeps all GPUs busy even when models are distributed across servers.

See [ADR-002](adr/002-fan-out-pattern-as-core.md) for rationale.

## Battery Execution: Pipelined Judging

When a judge model is selected, BatteryRunner uses **pipelined execution** â€” judge tasks are submitted eagerly as inference results complete, rather than waiting for all inference to finish first.

```
Without pipelining (original two-phase, ADR-008):
  Phase 1: [inference][inference][inference][inference]
  Phase 2:                                              [judge][judge][judge][judge]

With pipelining:
  GPU0:    [inference][inference][judge][judge][judge]    â† GPU0 idles early, starts judging
  GPU1:    [inference][inference][inference][inference]   â† GPU1 still doing heavy models
```

The `current_model` drain guard on `ServerPool` is the enabler â€” judge tasks queue in the dispatcher until a server drains its inference model. No priority queues or server affinity needed.

Key methods in `battery.py`:
- `_execute_pipelined()` â€” tracks `inference_tasks` and `judge_tasks` in separate sets
- `_inference_then_judge()` â€” wraps `_execute_test()`, submits judge task on success
- `_execute_inference_phase()` â€” used when no judge model (zero overhead)

When no judge model is set, `_execute_inference_phase()` runs directly with no pipelining overhead.

See [ADR-008](adr/ADR-008-judge-scheduling-strategy.md) for the evolution from two-phase to pipelined scheduling.

## Consistency Testing

`ConsistencyRunner` (in `consistency.py`) runs each (test, model) cell N times with different random seeds to identify models that produce inconsistent results.

| Status | Symbol | Meaning |
|--------|--------|---------|
| `CONSISTENT_PASS` | âœ“ | N/N runs passed |
| `CONSISTENT_FAIL` | âŒ | 0/N runs passed |
| `INCONSISTENT` | ğŸŸ£ 3/5 | Some runs passed, some failed |
| `PENDING` | â³ 2/5 | Not all runs complete |

Key types:
- `CellAggregate` â€” aggregated results for one (test, model) cell across N runs
- `ConsistencyRun` â€” state model (like `BatteryRun` but stores aggregates)
- `ConsistencyRunner` â€” orchestrator with same pipelined judging as `BatteryRunner`

See [ADR-010](adr/ADR-010-consistency-runner.md) for rationale.

## Architecture Decision Records

| ADR | Decision |
|-----|----------|
| [001](adr/001-use-existing-benchmarks.md) | Use existing benchmarks (BFCL, Inspect AI) instead of custom eval schema |
| [002](adr/002-fan-out-pattern-as-core.md) | Fan-out pattern as core architectural abstraction |
| [003](adr/003-openai-compatible-api.md) | OpenAI-compatible API as sole integration layer |
| [006](adr/006-adapter-resource-ownership.md) | Adapters own their resource management (ServerPool internal to LMStudioAdapter) |
| [007](adr/ADR-007-inference-task-schema.md) | InferenceTask schema for adapter interface |
| [008](adr/ADR-008-judge-scheduling-strategy.md) | Pipelined judge scheduling for multi-GPU efficiency |
| [009](adr/ADR-009-interactive-battery-grid.md) | Dismissible dialog for battery grid cell detail |
| [010](adr/ADR-010-consistency-runner.md) | Multi-run consistency analysis (proposed) |
| [011](adr/ADR-011-embedding-based-validation.md) | Embedding-based semantic validation (proposed) |
| [012](adr/ADR-012-compare-to-battery-export.md) | Compare to Battery export pipeline (proposed) |
